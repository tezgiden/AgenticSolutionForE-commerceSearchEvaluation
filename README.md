# Agentic E-commerce Search Evaluation Solution

This project implements an agentic solution to automatically search an e-commerce website (tundrafmp.com), scrape the results, and evaluate their relevance using a locally running Large Language Model (LLM) via Ollama.

## Project Structure

```
agentic_search_solution/
├── Dockerfile                # Docker configuration for the application
├── requirements.txt          # Python dependencies
├── scraper.py                # Web scraping module (Selenium)
├── llm_evaluator.py          # LLM interaction and evaluation module (Ollama)
├── main.py                   # Main orchestrator script
├── test_solution.py          # Unit and integration tests
├── requirements.md           # Detailed requirements specification
├── architecture.md           # System architecture design
├── prompts.md                # LLM prompt templates
├── infrastructure.md         # AWS infrastructure design document
├── cdk_infrastructure.md     # AWS CDK code and deployment instructions
├── todo.md                   # Task checklist (internal use)
└── final_evaluation_results.json # Example output file generated by main.py
```

## Features

-   Automated web scraping of search results from tundrafmp.com using Selenium.
-   Dynamic classification of search queries into types: `english_word`, `part_number`, `multiple_terms`.
-   Relevance evaluation of search results using a local LLM (via Ollama).
-   Customizable prompts tailored to different search query types.
-   Modular architecture with separate components for scraping, evaluation, and orchestration.
-   Example AWS deployment architecture using EC2 and Docker, with Infrastructure as Code (IaC) using AWS CDK.
-   Unit and integration tests for key components.

## Architecture Overview

The system consists of:
1.  **Orchestrator (`main.py`):** Manages the workflow, taking search queries, calling the scraper and evaluator, and compiling results.
2.  **Web Scraper (`scraper.py`):** Uses Selenium to interact with tundrafmp.com, perform searches, and extract product details (title, URL, SKU, price).
3.  **LLM Evaluator (`llm_evaluator.py`):** Classifies search queries, formats prompts based on query type, interacts with the Ollama API, and parses the LLM's relevance evaluation.
4.  **Local LLM:** An Ollama instance running a suitable model (e.g., Llama 3, Mistral).

(Refer to `architecture.md` for a detailed diagram and component descriptions.)

## LLM Prompts

Specific prompts are used based on the classified search type to guide the LLM in evaluating relevance:
-   **English Word:** Focuses on contextual relevance.
-   **Part Number:** Focuses on exact or partial matches of the part number.
-   **Multiple Terms:** Focuses on matching the combination of terms (product type, brand, application, etc.).

(Refer to `prompts.md` for the full prompt templates.)

## Setup and Usage (Local)

1.  **Prerequisites:**
    *   Python 3.10+
    *   Docker
    *   Ollama installed and running (e.g., `ollama serve`). Download a model: `ollama pull llama3` (or your preferred model).
    *   Google Chrome browser installed (for Selenium).

2.  **Clone the Repository (or download the files):**
    ```bash
    # Assuming files are in agentic_search_solution directory
    cd agentic_search_solution
    ```

3.  **Install Python Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(This will install `selenium`, `requests`, `webdriver-manager`, etc.)*

4.  **Ensure Ollama is Running:**
    *   Verify Ollama is accessible (usually at `http://localhost:11434`).
    *   Make sure the model specified in `llm_evaluator.py` (default: `llama3`) is available in Ollama.

5.  **Run the Application:**
    ```bash
    python main.py
    ```
    *   The script will execute the predefined search tasks, perform scraping, evaluate results using Ollama, and save the output to `final_evaluation_results.json`.

6.  **Run Tests (Optional):**
    ```bash
    
    ```
    *(Note: Some tests mock external dependencies like the web driver and Ollama API.)*

## AWS Deployment

The recommended approach involves running both the application and Ollama within Docker containers on a single EC2 instance.

1.  **Infrastructure:** Defined using AWS CDK in `cdk_infrastructure.md`. This script provisions a VPC, EC2 instance, Security Group, IAM Role, and an optional S3 bucket.
2.  **Deployment Steps:**
    *   Install AWS CDK and configure AWS credentials.
    *   Deploy the CDK stack (`cdk deploy`).
    *   SSH into the created EC2 instance.
    *   The CDK user data script attempts to install Docker, clone the code, pull/run Ollama, and build the app container. You might need to manually run the application using the provided `start-app.sh` script or adapt the user data.

(Refer to `infrastructure.md` and `cdk_infrastructure.md` for detailed architecture and deployment instructions.)

## Code Files

-   `scraper.py`: Implements web scraping logic.
-   `llm_evaluator.py`: Implements LLM interaction and evaluation logic.
-   `main.py`: Orchestrates the end-to-end process.
-   `test_solution.py`: Contains unit and integration tests.
-   `Dockerfile`: Defines the Docker image for the application.
-   `requirements.txt`: Lists Python dependencies.

## Future Improvements

-   **Error Handling:** Enhance error handling in the scraper (e.g., retries, handling different page layouts).
-   **Pagination:** Implement logic in the scraper to handle multiple pages of search results.
-   **Description Scraping:** Add functionality to navigate to individual product pages to scrape detailed descriptions if needed for better evaluation.
-   **Configuration:** Move hardcoded configurations (URLs, selectors, model names) to a separate config file or environment variables.
-   **Input Methods:** Support reading search tasks from files (e.g., CSV, JSON) or an API endpoint.
-   **Parallel Processing:** Implement parallel execution of search tasks for faster processing.
-   **Refine Tests:** Fix the remaining failing test in `test_solution.py` related to scraper mocking.

